A0AF_famine_test rice
2005
2005 ['2005'] 2005
predicted_years: ['2005'] max_epochs: 1500 batch_size: 128 learning_rate 0.0001 loss_func_metric: RMSE seed: 123456 lr_milestones_list: [20, 50, 600, 800]
loading data/ALIM128F64DATASET_rice.csv Thu Apr 27 13:57:30 2023
data/ALIM128F64DATASET_rice.csv loaded Thu Apr 27 13:57:40 2023
<class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'> <class 'numpy.int64'>
['county', 'year', 'month', 'gstage', 'time_idx', 'actuals', 'rice_sownarea', 'avg_rice_sownarea', 'med_rice_sownarea', 'rice_yieldval', 'avg_rice_yieldval', 'med_rice_yieldval', 'rice_yield', 'avg_rice_yield', 'med_rice_yield']
['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
2005 <class 'str'>
Years to train: ['2003', '2004', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']
Years to valid: ['2005']
--------check 2008----------------------
Years to train: ['2003' '2004' '2006' '2007' '2009' '2010' '2011' '2012' '2013' '2014'
 '2015' '2016' '2017' '2018']
Years to valid: ['2005']
Years to valid: ['2005']
------------------------------
DATA_VAL: ['0'] (918, 2809)
DATA_VAL: ['0'] (918, 2809)
DATA_VAL: ['0'] (34, 2809)
Augmentation for years list: [2003, 2004, 2006, 2007, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018] by NSAMPLES=1 and YEARS_MAX_LENGTH=5
type(self.data_train[time_idx]) <class 'pandas.core.series.Series'>
time_idx [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135]
sample ['0']
county ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '11' '12' '13' '14' '15' '16'
 '17' '19' '21' '22' '23' '24' '25' '26' '27' '29' '30']
df[year].unique() ['2003' '2017' '2006' '2005']
df[time_idx].to_numpy() [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135]
self.val_years[0]: ['2005'] ['2003' '2017' '2006' '2005' '2015' '2004' '2007' '2013' '2018' '2010'
 '2012' '2009' '2014' '2016' '2011']
self.val_years[0]: 2005 ['2003' '2017' '2006' '2005']
ival_years: ['2005', '2005', '2005']
30 4
last_year: ['2005']
34 [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
34 [102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119
 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135]
[102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119
 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135]
Dataframe size: 77.24 Mb
DataGenerator done...
Experiment exist: EXP_[A0AF_famine_test]-Cr[rice]-KF[2005]-BS[128]]
Set another exp_name...
Set basic filenames self.name_for_files: EXP_[A0AF_famine_test]-Cr[rice]-KF[2005]-BS[128]]
avg_med: ['avg_rice_yield', 'actuals']
training mx_epochs, TimeSeriesDataSet: 1500 Thu Apr 27 13:58:02 2023
D1: known-unknown go --------------------------
D2: --------------------------
Thu Apr 27 13:58:03 2023
training & validation TimeSeriesDataSet loaded Thu Apr 27 13:58:03 2023
self.train_dataloader: 21
self.val_dataloader: 1
self.test_dataloader: 1
Thu Apr 27 13:58:05 2023
Baseline: tensor(0.0251, device='cuda:0')
Baseline: <class 'tuple'> <class 'torch.Tensor'> <class 'tuple'>
Baseline: (tensor([[0.8400, 0.8400, 0.8400, 0.8400, 0.6250, 0.6250, 0.6250, 0.6250],
        [0.7600, 0.7600, 0.7600, 0.7600, 0.7293, 0.7293, 0.7293, 0.7293],
        [0.6400, 0.6400, 0.6400, 0.6400, 0.6129, 0.6129, 0.6129, 0.6129],
        [0.6200, 0.6200, 0.6200, 0.6200, 0.5851, 0.5851, 0.5851, 0.5851],
        [0.6400, 0.6400, 0.6400, 0.6400, 0.6427, 0.6427, 0.6427, 0.6427],
        [0.8200, 0.8200, 0.8200, 0.8200, 0.7997, 0.7997, 0.7997, 0.7997],
        [0.7400, 0.7400, 0.7400, 0.7400, 0.7040, 0.7040, 0.7040, 0.7040],
        [0.8400, 0.8400, 0.8400, 0.8400, 0.8387, 0.8387, 0.8387, 0.8387],
        [0.7000, 0.7000, 0.7000, 0.7000, 0.6773, 0.6773, 0.6773, 0.6773],
        [0.6000, 0.6000, 0.6000, 0.6000, 0.5692, 0.5692, 0.5692, 0.5692],
        [0.6400, 0.6400, 0.6400, 0.6400, 0.5882, 0.5882, 0.5882, 0.5882],
        [0.7200, 0.7200, 0.7200, 0.7200, 0.6973, 0.6973, 0.6973, 0.6973],
        [0.7500, 0.7500, 0.7500, 0.7500, 0.7215, 0.7215, 0.7215, 0.7215],
        [0.5700, 0.5700, 0.5700, 0.5700, 0.6552, 0.6552, 0.6552, 0.6552],
        [0.6000, 0.6000, 0.6000, 0.6000, 0.6208, 0.6208, 0.6208, 0.6208],
        [0.5500, 0.5500, 0.5500, 0.5500, 0.5392, 0.5392, 0.5392, 0.5392],
        [0.6800, 0.6800, 0.6800, 0.6800, 0.6064, 0.6064, 0.6064, 0.6064],
        [0.7100, 0.7100, 0.7100, 0.7100, 0.8059, 0.8059, 0.8059, 0.8059],
        [0.8300, 0.8300, 0.8300, 0.8300, 0.8570, 0.8570, 0.8570, 0.8570],
        [0.5100, 0.5100, 0.5100, 0.5100, 0.3358, 0.3358, 0.3358, 0.3358],
        [0.8300, 0.8300, 0.8300, 0.8300, 0.7765, 0.7765, 0.7765, 0.7765],
        [0.6800, 0.6800, 0.6800, 0.6800, 0.7359, 0.7359, 0.7359, 0.7359],
        [0.7700, 0.7700, 0.7700, 0.7700, 0.7328, 0.7328, 0.7328, 0.7328],
        [0.7800, 0.7800, 0.7800, 0.7800, 0.7237, 0.7237, 0.7237, 0.7237],
        [0.7000, 0.7000, 0.7000, 0.7000, 0.6796, 0.6796, 0.6796, 0.6796],
        [0.8300, 0.8300, 0.8300, 0.8300, 0.7592, 0.7592, 0.7592, 0.7592],
        [0.8200, 0.8200, 0.8200, 0.8200, 0.7726, 0.7726, 0.7726, 0.7726]],
       device='cuda:0'), None)
Baseline: tensor(0.0251, device='cuda:0')
Baseline: Thu Apr 27 13:58:08 2023
Thu Apr 27 13:58:08 2023
CycicLR: 0.0001 0.01 350 2600 triangular2
on_fit_start: 0.0001
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking:   0%|                                                                                        | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|                                                                           | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.84it/s]                                                                                                                                    Training: 0it [00:00, ?it/s]on_train_epoch_start: 0.0001282857142857231
Training:   0%|                                                                                              | 0/21 [00:00<?, ?it/s]Epoch 0:   0%|                                                                                               | 0/21 [00:00<?, ?it/s]Epoch 0:   5%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                  | 1/21 [00:00<00:16,  1.20it/s]Epoch 0:   5%|â–ˆâ–ˆâ–Œ                                                   | 1/21 [00:00<00:16,  1.20it/s, v_num=77, train_loss_step=0.064]Epoch 0:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 2/21 [00:00<00:08,  2.20it/s, v_num=77, train_loss_step=0.064]Epoch 0:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 2/21 [00:00<00:08,  2.20it/s, v_num=77, train_loss_step=0.027]Epoch 0:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                              | 3/21 [00:00<00:05,  3.12it/s, v_num=77, train_loss_step=0.027]Epoch 0:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 3/21 [00:00<00:05,  3.12it/s, v_num=77, train_loss_step=0.0298]Epoch 0:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 4/21 [00:01<00:04,  3.89it/s, v_num=77, train_loss_step=0.0298]Epoch 0:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 4/21 [00:01<00:04,  3.89it/s, v_num=77, train_loss_step=0.0252]Epoch 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 5/21 [00:01<00:03,  4.59it/s, v_num=77, train_loss_step=0.0252]Epoch 0:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 5/21 [00:01<00:03,  4.59it/s, v_num=77, train_loss_step=0.0978]Epoch 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 6/21 [00:01<00:02,  5.26it/s, v_num=77, train_loss_step=0.0978]Epoch 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 6/21 [00:01<00:02,  5.26it/s, v_num=77, train_loss_step=0.0199]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 7/21 [00:01<00:02,  5.82it/s, v_num=77, train_loss_step=0.0199]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 7/21 [00:01<00:02,  5.82it/s, v_num=77, train_loss_step=0.0195]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 8/21 [00:01<00:02,  6.36it/s, v_num=77, train_loss_step=0.0195]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 8/21 [00:01<00:02,  6.36it/s, v_num=77, train_loss_step=0.0484]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 9/21 [00:01<00:01,  6.73it/s, v_num=77, train_loss_step=0.0484]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 9/21 [00:01<00:01,  6.72it/s, v_num=77, train_loss_step=0.070]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 10/21 [00:01<00:01,  7.14it/s, v_num=77, train_loss_step=0.070]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 10/21 [00:01<00:01,  7.13it/s, v_num=77, train_loss_step=0.0232]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 11/21 [00:01<00:01,  7.58it/s, v_num=77, train_loss_step=0.0232]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 11/21 [00:01<00:01,  7.58it/s, v_num=77, train_loss_step=0.0253]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 12/21 [00:01<00:01,  7.98it/s, v_num=77, train_loss_step=0.0253]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 12/21 [00:01<00:01,  7.98it/s, v_num=77, train_loss_step=0.0437]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 13/21 [00:01<00:00,  8.37it/s, v_num=77, train_loss_step=0.0437]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 13/21 [00:01<00:00,  8.37it/s, v_num=77, train_loss_step=0.0366]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 14/21 [00:01<00:00,  8.74it/s, v_num=77, train_loss_step=0.0366]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 14/21 [00:01<00:00,  8.74it/s, v_num=77, train_loss_step=0.0368]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 15/21 [00:01<00:00,  9.06it/s, v_num=77, train_loss_step=0.0368]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 15/21 [00:01<00:00,  9.05it/s, v_num=77, train_loss_step=0.0344]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 16/21 [00:01<00:00,  9.36it/s, v_num=77, train_loss_step=0.0344]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 16/21 [00:01<00:00,  9.35it/s, v_num=77, train_loss_step=0.0252]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 17/21 [00:01<00:00,  9.64it/s, v_num=77, train_loss_step=0.0252]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 17/21 [00:01<00:00,  9.63it/s, v_num=77, train_loss_step=0.0376]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 18/21 [00:01<00:00,  9.88it/s, v_num=77, train_loss_step=0.0376]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 18/21 [00:01<00:00,  9.88it/s, v_num=77, train_loss_step=0.0616]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/21 [00:01<00:00, 10.12it/s, v_num=77, train_loss_step=0.0616]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/21 [00:01<00:00, 10.12it/s, v_num=77, train_loss_step=0.0295]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 20/21 [00:01<00:00, 10.35it/s, v_num=77, train_loss_step=0.0295]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 20/21 [00:01<00:00, 10.34it/s, v_num=77, train_loss_step=0.0322]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 10.52it/s, v_num=77, train_loss_step=0.0322]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 10.52it/s, v_num=77, train_loss_step=0.0546]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|                                                                                             | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|                                                                                | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.77it/s][AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:02<00:00,  8.15it/s, v_num=77, train_loss_step=0.0546, val_loss=0.0465]
                                                                                                                                    [A{'state': TrainerState(status=<TrainerStatus.RUNNING: 'running'>, fn=<TrainerFn.FITTING: 'fit'>, stage=<RunningStage.TRAINING: 'train'>), 'barebones': False, '_data_connector': <lightning.pytorch.trainer.connectors.data_connector._DataConnector object at 0x7f3a9e42feb0>, '_accelerator_connector': <lightning.pytorch.trainer.connectors.accelerator_connector._AcceleratorConnector object at 0x7f3a9e42fe20>, '_logger_connector': <lightning.pytorch.trainer.connectors.logger_connector.logger_connector._LoggerConnector object at 0x7f3a9e42fc70>, '_callback_connector': <lightning.pytorch.trainer.connectors.callback_connector._CallbackConnector object at 0x7f3a9e42fc40>, '_checkpoint_connector': <lightning.pytorch.trainer.connectors.checkpoint_connector._CheckpointConnector object at 0x7f3a9e42f8e0>, '_signal_connector': <lightning.pytorch.trainer.connectors.signal_connector._SignalConnector object at 0x7f3a9e42f3d0>, 'fit_loop': <lightning.pytorch.loops.fit_loop._FitLoop object at 0x7f3a9e42f1f0>, 'validate_loop': <lightning.pytorch.loops.evaluation_loop._EvaluationLoop object at 0x7f3a9e42f370>, 'test_loop': <lightning.pytorch.loops.evaluation_loop._EvaluationLoop object at 0x7f3a9e42f250>, 'predict_loop': <lightning.pytorch.loops.prediction_loop._PredictionLoop object at 0x7f3a9e42fb50>, 'accumulate_grad_batches': 1, '_default_root_dir': '/hy-tmp', 'callbacks': [<utils.FineTuneLearningRateFinder_CyclicLR object at 0x7f3a9e420250>, <lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor object at 0x7f3a9e4207c0>, <utils.ActualVsPredictedCallback object at 0x7f3a9f35ddc0>, <lightning.pytorch.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x7f3a9e42f760>, <lightning.pytorch.callbacks.model_summary.ModelSummary object at 0x7f3a9e42ffd0>, <lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f3a9f35dc70>], 'datamodule': None, 'check_val_every_n_epoch': 1, 'reload_dataloaders_every_n_epochs': 1, 'gradient_clip_val': 0.2, 'gradient_clip_algorithm': None, '_detect_anomaly': False, 'should_stop': False, 'profiler': <lightning.pytorch.profilers.base.PassThroughProfiler object at 0x7f3a9e803ac0>, '_loggers': [<lightning.pytorch.loggers.tensorboard.TensorBoardLogger object at 0x7f3a9e81ac10>], 'log_every_n_steps': 1, 'fast_dev_run': False, 'overfit_batches': 0.0, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'limit_test_batches': 1.0, 'limit_predict_batches': 1.0, 'num_sanity_val_steps': 2, 'val_check_interval': 1.0, 'val_check_batch': 21}
{'_log_hyperparams': True, '_hparams_name': 'loss', '_hparams': "attention_head_size":               4
"categorical_groups":                {}
"causal_attention":                  True
"dropout":                           0.3
"embedding_labels":                  {'month': {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8}, 'gstage': {'no': 0, 'yield': 1}}
"embedding_paddings":                []
"embedding_sizes":                   {'month': (9, 5), 'gstage': (2, 1)}
"hidden_continuous_size":            8
"hidden_continuous_sizes":           {}
"hidden_size":                       16
"learning_rate":                     0.0001
"log_gradient_flow":                 False
"log_interval":                      -1
"log_val_interval":                  -1
"logging_metrics":                   ModuleList(
  (0): SMAPE()
  (1): MAE()
  (2): RMSE()
  (3): MAPE()
)
"loss":                              RMSE()
"lstm_layers":                       1
"max_encoder_length":                26
"monotone_constaints":               {}
"optimizer":                         sgd
"optimizer_params":                  None
"output_size":                       1
"output_transformer":                GroupNormalizer(
	method='standard',
	groups=['county', 'sample'],
	center=True,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)
"reduce_on_plateau_min_lr":          1e-05
"reduce_on_plateau_patience":        1000
"reduce_on_plateau_reduction":       2.0
"share_single_variable_networks":    False
"static_categoricals":               []
"static_reals":                      ['encoder_length', 'rice_yield_center', 'rice_yield_scale']
"time_varying_categoricals_decoder": ['month', 'gstage']
"time_varying_categoricals_encoder": ['month', 'gstage']
"time_varying_reals_decoder":        ['avg_rice_yield', 'actuals', 'relative_time_idx']
"time_varying_reals_encoder":        ['avg_rice_yield', 'actuals', 'relative_time_idx', 'avg_rice_yield', 'actuals']
"weight_decay":                      0.0
"x_categoricals":                    ['month', 'gstage']
"x_reals":                           ['encoder_length', 'rice_yield_center', 'rice_yield_scale', 'avg_rice_yield', 'actuals', 'relative_time_idx', 'avg_rice_yield', 'actuals'], '_hparams_initial': "attention_head_size":               4
"categorical_groups":                {}
"causal_attention":                  True
"dropout":                           0.3
"embedding_labels":                  {'month': {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7, '9': 8}, 'gstage': {'no': 0, 'yield': 1}}
"embedding_paddings":                []
"embedding_sizes":                   {'month': (9, 5), 'gstage': (2, 1)}
"hidden_continuous_size":            8
"hidden_continuous_sizes":           {}
"hidden_size":                       16
"learning_rate":                     0.0001
"log_gradient_flow":                 False
"log_interval":                      -1
"log_val_interval":                  None
"logging_metrics":                   ModuleList(
  (0): SMAPE()
  (1): MAE()
  (2): RMSE()
  (3): MAPE()
)
"loss":                              RMSE()
"lstm_layers":                       1
"max_encoder_length":                26
"monotone_constaints":               {}
"optimizer":                         sgd
"optimizer_params":                  None
"output_size":                       1
"output_transformer":                GroupNormalizer(
	method='standard',
	groups=['county', 'sample'],
	center=True,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
)
"reduce_on_plateau_min_lr":          1e-05
"reduce_on_plateau_patience":        1000
"reduce_on_plateau_reduction":       2.0
"share_single_variable_networks":    False
"static_categoricals":               []
"static_reals":                      ['encoder_length', 'rice_yield_center', 'rice_yield_scale']
"time_varying_categoricals_decoder": ['month', 'gstage']
"time_varying_categoricals_encoder": ['month', 'gstage']
"time_varying_reals_decoder":        ['avg_rice_yield', 'actuals', 'relative_time_idx']
"time_varying_reals_encoder":        ['avg_rice_yield', 'actuals', 'relative_time_idx', 'avg_rice_yield', 'actuals']
"weight_decay":                      0.0
"x_categoricals":                    ['month', 'gstage']
"x_reals":                           ['encoder_length', 'rice_yield_center', 'rice_yield_scale', 'avg_rice_yield', 'actuals', 'relative_time_idx', 'avg_rice_yield', 'actuals'], 'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict([(2, <function state_dict_hook at 0x7f3c28cc2430>)]), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict([(3, <torch.nn.modules.module._WrappedHook object at 0x7f3a9b574340>)]), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('loss', RMSE()), ('logging_metrics', ModuleList(
  (0): SMAPE()
  (1): MAE()
  (2): RMSE()
  (3): MAPE()
)), ('input_embeddings', MultiEmbedding(
  (embeddings): ModuleDict(
    (month): Embedding(9, 5)
    (gstage): Embedding(2, 1)
  )
)), ('prescalers', ModuleDict(
  (encoder_length): Linear(in_features=1, out_features=8, bias=True)
  (rice_yield_center): Linear(in_features=1, out_features=8, bias=True)
  (rice_yield_scale): Linear(in_features=1, out_features=8, bias=True)
  (avg_rice_yield): Linear(in_features=1, out_features=8, bias=True)
  (actuals): Linear(in_features=1, out_features=8, bias=True)
  (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)
)), ('static_variable_selection', VariableSelectionNetwork(
  (flattened_grn): GatedResidualNetwork(
    (resample_norm): ResampleNorm(
      (resample): TimeDistributedInterpolation()
      (gate): Sigmoid()
      (norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)
    )
    (fc1): Linear(in_features=24, out_features=3, bias=True)
    (elu): ELU(alpha=1.0)
    (fc2): Linear(in_features=3, out_features=3, bias=True)
    (gate_norm): GateAddNorm(
      (glu): GatedLinearUnit(
        (dropout): Dropout(p=0.3, inplace=False)
        (fc): Linear(in_features=3, out_features=6, bias=True)
      )
      (add_norm): AddNorm(
        (norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (single_variable_grns): ModuleDict(
    (encoder_length): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (rice_yield_center): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (rice_yield_scale): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (prescalers): ModuleDict(
    (encoder_length): Linear(in_features=1, out_features=8, bias=True)
    (rice_yield_center): Linear(in_features=1, out_features=8, bias=True)
    (rice_yield_scale): Linear(in_features=1, out_features=8, bias=True)
  )
  (softmax): Softmax(dim=-1)
)), ('encoder_variable_selection', VariableSelectionNetwork(
  (flattened_grn): GatedResidualNetwork(
    (resample_norm): ResampleNorm(
      (resample): TimeDistributedInterpolation()
      (gate): Sigmoid()
      (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
    )
    (fc1): Linear(in_features=30, out_features=5, bias=True)
    (elu): ELU(alpha=1.0)
    (context): Linear(in_features=16, out_features=5, bias=False)
    (fc2): Linear(in_features=5, out_features=5, bias=True)
    (gate_norm): GateAddNorm(
      (glu): GatedLinearUnit(
        (dropout): Dropout(p=0.3, inplace=False)
        (fc): Linear(in_features=5, out_features=10, bias=True)
      )
      (add_norm): AddNorm(
        (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (single_variable_grns): ModuleDict(
    (month): ResampleNorm(
      (resample): TimeDistributedInterpolation()
      (gate): Sigmoid()
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
    (gstage): ResampleNorm(
      (resample): TimeDistributedInterpolation()
      (gate): Sigmoid()
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
    (avg_rice_yield): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (actuals): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (relative_time_idx): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (prescalers): ModuleDict(
    (avg_rice_yield): Linear(in_features=1, out_features=8, bias=True)
    (actuals): Linear(in_features=1, out_features=8, bias=True)
    (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)
  )
  (softmax): Softmax(dim=-1)
)), ('decoder_variable_selection', VariableSelectionNetwork(
  (flattened_grn): GatedResidualNetwork(
    (resample_norm): ResampleNorm(
      (resample): TimeDistributedInterpolation()
      (gate): Sigmoid()
      (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
    )
    (fc1): Linear(in_features=30, out_features=5, bias=True)
    (elu): ELU(alpha=1.0)
    (context): Linear(in_features=16, out_features=5, bias=False)
    (fc2): Linear(in_features=5, out_features=5, bias=True)
    (gate_norm): GateAddNorm(
      (glu): GatedLinearUnit(
        (dropout): Dropout(p=0.3, inplace=False)
        (fc): Linear(in_features=5, out_features=10, bias=True)
      )
      (add_norm): AddNorm(
        (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (single_variable_grns): ModuleDict(
    (month): ResampleNorm(
      (resample): TimeDistributedInterpolation()
      (gate): Sigmoid()
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
    (gstage): ResampleNorm(
      (resample): TimeDistributedInterpolation()
      (gate): Sigmoid()
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
    (avg_rice_yield): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (actuals): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (relative_time_idx): GatedResidualNetwork(
      (resample_norm): ResampleNorm(
        (resample): TimeDistributedInterpolation()
        (gate): Sigmoid()
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (fc1): Linear(in_features=8, out_features=8, bias=True)
      (elu): ELU(alpha=1.0)
      (fc2): Linear(in_features=8, out_features=8, bias=True)
      (gate_norm): GateAddNorm(
        (glu): GatedLinearUnit(
          (dropout): Dropout(p=0.3, inplace=False)
          (fc): Linear(in_features=8, out_features=32, bias=True)
        )
        (add_norm): AddNorm(
          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (prescalers): ModuleDict(
    (avg_rice_yield): Linear(in_features=1, out_features=8, bias=True)
    (actuals): Linear(in_features=1, out_features=8, bias=True)
    (relative_time_idx): Linear(in_features=1, out_features=8, bias=True)
  )
  (softmax): Softmax(dim=-1)
)), ('static_context_variable_selection', GatedResidualNetwork(
  (fc1): Linear(in_features=16, out_features=16, bias=True)
  (elu): ELU(alpha=1.0)
  (fc2): Linear(in_features=16, out_features=16, bias=True)
  (gate_norm): GateAddNorm(
    (glu): GatedLinearUnit(
      (dropout): Dropout(p=0.3, inplace=False)
      (fc): Linear(in_features=16, out_features=32, bias=True)
    )
    (add_norm): AddNorm(
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
  )
)), ('static_context_initial_hidden_lstm', GatedResidualNetwork(
  (fc1): Linear(in_features=16, out_features=16, bias=True)
  (elu): ELU(alpha=1.0)
  (fc2): Linear(in_features=16, out_features=16, bias=True)
  (gate_norm): GateAddNorm(
    (glu): GatedLinearUnit(
      (dropout): Dropout(p=0.3, inplace=False)
      (fc): Linear(in_features=16, out_features=32, bias=True)
    )
    (add_norm): AddNorm(
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
  )
)), ('static_context_initial_cell_lstm', GatedResidualNetwork(
  (fc1): Linear(in_features=16, out_features=16, bias=True)
  (elu): ELU(alpha=1.0)
  (fc2): Linear(in_features=16, out_features=16, bias=True)
  (gate_norm): GateAddNorm(
    (glu): GatedLinearUnit(
      (dropout): Dropout(p=0.3, inplace=False)
      (fc): Linear(in_features=16, out_features=32, bias=True)
    )
    (add_norm): AddNorm(
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
  )
)), ('static_context_enrichment', GatedResidualNetwork(
  (fc1): Linear(in_features=16, out_features=16, bias=True)
  (elu): ELU(alpha=1.0)
  (fc2): Linear(in_features=16, out_features=16, bias=True)
  (gate_norm): GateAddNorm(
    (glu): GatedLinearUnit(
      (dropout): Dropout(p=0.3, inplace=False)
      (fc): Linear(in_features=16, out_features=32, bias=True)
    )
    (add_norm): AddNorm(
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
  )
)), ('lstm_encoder', LSTM(16, 16, batch_first=True)), ('lstm_decoder', LSTM(16, 16, batch_first=True)), ('post_lstm_gate_encoder', GatedLinearUnit(
  (dropout): Dropout(p=0.3, inplace=False)
  (fc): Linear(in_features=16, out_features=32, bias=True)
)), ('post_lstm_gate_decoder', GatedLinearUnit(
  (dropout): Dropout(p=0.3, inplace=False)
  (fc): Linear(in_features=16, out_features=32, bias=True)
)), ('post_lstm_add_norm_encoder', AddNorm(
  (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
)), ('post_lstm_add_norm_decoder', AddNorm(
  (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
)), ('static_enrichment', GatedResidualNetwork(
  (fc1): Linear(in_features=16, out_features=16, bias=True)
  (elu): ELU(alpha=1.0)
  (context): Linear(in_features=16, out_features=16, bias=False)
  (fc2): Linear(in_features=16, out_features=16, bias=True)
  (gate_norm): GateAddNorm(
    (glu): GatedLinearUnit(
      (dropout): Dropout(p=0.3, inplace=False)
      (fc): Linear(in_features=16, out_features=32, bias=True)
    )
    (add_norm): AddNorm(
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
  )
)), ('multihead_attn', InterpretableMultiHeadAttention(
  (dropout): Dropout(p=0.3, inplace=False)
  (v_layer): Linear(in_features=16, out_features=4, bias=True)
  (q_layers): ModuleList(
    (0-3): 4 x Linear(in_features=16, out_features=4, bias=True)
  )
  (k_layers): ModuleList(
    (0-3): 4 x Linear(in_features=16, out_features=4, bias=True)
  )
  (attention): ScaledDotProductAttention(
    (softmax): Softmax(dim=2)
  )
  (w_h): Linear(in_features=4, out_features=16, bias=False)
)), ('post_attn_gate_norm', GateAddNorm(
  (glu): GatedLinearUnit(
    (dropout): Dropout(p=0.3, inplace=False)
    (fc): Linear(in_features=16, out_features=32, bias=True)
  )
  (add_norm): AddNorm(
    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
  )
)), ('pos_wise_ff', GatedResidualNetwork(
  (fc1): Linear(in_features=16, out_features=16, bias=True)
  (elu): ELU(alpha=1.0)
  (fc2): Linear(in_features=16, out_features=16, bias=True)
  (gate_norm): GateAddNorm(
    (glu): GatedLinearUnit(
      (dropout): Dropout(p=0.3, inplace=False)
      (fc): Linear(in_features=16, out_features=32, bias=True)
    )
    (add_norm): AddNorm(
      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
  )
)), ('pre_output_gate_norm', GateAddNorm(
  (glu): GatedLinearUnit(
    (fc): Linear(in_features=16, out_features=32, bias=True)
  )
  (add_norm): AddNorm(
    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
  )
)), ('output_layer', Linear(in_features=16, out_features=1, bias=True))]), 'prepare_data_per_node': True, 'allow_zero_length_dataloader_with_multiple_devices': False, '_dtype': torch.float32, '_device': device(type='cuda', index=0), '_trainer': <lightning.pytorch.trainer.trainer.Trainer object at 0x7f3a9e42f640>, '_example_input_array': None, '_current_fx_name': 'on_train_epoch_end', '_automatic_optimization': True, '_param_requires_grad_state': {}, '_metric_attributes': None, '_compiler_ctx': None, '_fabric': None, '_fabric_optimizers': [], 'output_transformer': GroupNormalizer(
	method='standard',
	groups=['county', 'sample'],
	center=True,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
), 'optimizer': 'sgd', 'hparams_special': [], 'training_step_outputs': [{'loss': tensor(0.0640, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0270, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0298, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0252, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0978, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0199, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0195, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0484, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0700, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0232, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0253, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0437, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0366, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0368, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0344, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0252, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0376, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0616, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0295, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0322, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}, {'loss': tensor(0.0546, device='cuda:0', grad_fn=<SqueezeBackward0>), 'n_samples': 128}], 'validation_step_outputs': [], 'testing_step_outputs': [], 'dataset_parameters': {'time_idx': 'time_idx', 'target': 'rice_yield', 'group_ids': ['county', 'sample'], 'weight': None, 'max_encoder_length': 26, 'min_encoder_length': 26, 'min_prediction_idx': 0, 'min_prediction_length': 8, 'max_prediction_length': 8, 'static_categoricals': [], 'static_reals': ['encoder_length', 'rice_yield_center', 'rice_yield_scale'], 'time_varying_known_categoricals': ['month', 'gstage'], 'time_varying_known_reals': ['avg_rice_yield', 'actuals', 'relative_time_idx'], 'time_varying_unknown_categoricals': [], 'time_varying_unknown_reals': ['avg_rice_yield', 'actuals'], 'variable_groups': {}, 'constant_fill_strategy': {}, 'allow_missing_timesteps': False, 'lags': {}, 'add_relative_time_idx': True, 'add_target_scales': True, 'add_encoder_length': True, 'target_normalizer': GroupNormalizer(
	method='standard',
	groups=['county', 'sample'],
	center=True,
	scale_by_group=False,
	transformation=None,
	method_kwargs={}
), 'categorical_encoders': {'__group_id__county': NaNLabelEncoder(add_nan=False, warn=True), '__group_id__sample': NaNLabelEncoder(add_nan=False, warn=True), 'county': NaNLabelEncoder(add_nan=False, warn=True), 'sample': NaNLabelEncoder(add_nan=False, warn=True), 'month': NaNLabelEncoder(add_nan=False, warn=True), 'gstage': NaNLabelEncoder(add_nan=False, warn=True)}, 'scalers': {'encoder_length': StandardScaler(), 'rice_yield_center': StandardScaler(), 'rice_yield_scale': StandardScaler(), 'avg_rice_yield': StandardScaler(), 'actuals': StandardScaler(), 'relative_time_idx': StandardScaler()}, 'randomize_length': None, 'predict_mode': False}, '_output_class': <class 'pytorch_forecasting.utils.TupleOutputMixIn.to_network_output.<locals>.Output'>}
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:03<00:00,  6.51it/s, v_num=77, train_loss_step=0.0546, val_loss=0.0465]                         
